{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MTGMU_6gN5G"
      },
      "source": [
        "# Simple Ring Reduce\n",
        "\n",
        "Here is the very simplest ring reduce. Our goal is to sum up n vectors in R^n. Here each worker holds a column xss[:][i] of the input matrix, and we're summing the columns together. We'll end up with the correct column-sum on the 2nd superdiagonal of the matrix. (It just works out that way; see the picture in the slides.)\n",
        "\n",
        "This is supposed to correspond to the illustration on [these](https://dlsys.cs.washington.edu/pdf/lecture11.pdf) slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4D6eDodBYvp",
        "outputId": "f32eb9a3-f19a-4445-86d9-7c0df4954586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Matrix:\n",
            "[[9 6 4 7 6]\n",
            " [3 5 8 2 9]\n",
            " [9 9 4 5 8]\n",
            " [9 6 9 1 8]\n",
            " [3 8 1 2 5]]\n",
            "\n",
            "Reduced Matrix:\n",
            "[[15  6 32 28 21]\n",
            " [16 13  8 27 25]\n",
            " [27 18  9  5 35]\n",
            " [33 24 18  9  8]\n",
            " [ 3 19 11 10  8]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def ring_reduce(xss):\n",
        "    n = len(xss)\n",
        "    assert n == len(xss[0]) # Assume the number of workers is equal to\n",
        "                            # the number of components\n",
        "    for i in range(n - 1):\n",
        "        for j in range(n):\n",
        "            # At time stage i, worker j:\n",
        "            #\n",
        "            # - sends data to worker (j - 1) % n in component (j - 1 +\n",
        "            #   i) % n and receives data from worker (j + 1) % n in\n",
        "            #   component (j + i) % n\n",
        "            #\n",
        "            donor = (j + 1) % n\n",
        "            component = (j + i) % n\n",
        "            received_data = xss[component][donor]\n",
        "\n",
        "            # - Does some reduction in the component it just\n",
        "            #   received, (j + i - 1) % n\n",
        "\n",
        "            xss[component][j] += received_data\n",
        "\n",
        "def create_matrix(n):\n",
        "    return np.random.randint(1, 10, size=(n, n))\n",
        "\n",
        "n = 5\n",
        "original_matrix = create_matrix(n)\n",
        "print(\"Original Matrix:\")\n",
        "print(original_matrix)\n",
        "matrix_to_modify = original_matrix.copy()\n",
        "ring_reduce(matrix_to_modify)\n",
        "\n",
        "print(\"\\nReduced Matrix:\")\n",
        "print(matrix_to_modify)\n",
        "\n",
        "# In our example, the correct results end up on the second\n",
        "# superdiagonal\n",
        "for i in range(n):\n",
        "    assert(sum(original_matrix[i]) == matrix_to_modify[i][(i + 2) % n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejI55YeUrPTQ"
      },
      "source": [
        "# Extending to Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-lh5Fk7rSLY"
      },
      "source": [
        "For reference, let's look at the top of page 4 of [FlashAttention2](https://arxiv.org/pdf/2307.08691.pdf), where they describe an iterative approach which loads blocks $S^{(i)}$ and $V^{(i)}$ from high-bandwidth memory one at a time, and incorporates them into their running approximation of their output vectors $\\mathbf{O}$. Once the algorithm here has loaded and incorporated all the blocks, the output vectors will be correct. (However, note that there are, I think, two typos in the algorithm, which I have pasted into the ring-attention channel.)\n",
        "\n",
        "In ring attention, we imagine this same stagewise computation taking place separately for each worker. Now instead of loading a blocks $S^{(i)}$ and $V^{(i)}$ from HBM at each stage, you receive at each stage a block of $\\mathbf{k}$'s and $\\mathbf{v}$'s from your neighbor. The $\\mathbf{v}$'s correspond exactly to the block $V^{(i)}$ loaded from bandwidth memory in flash attention. The $\\mathbf{k}$'s are not same as the block $S^{(i)}$, however; rather, they're some of the ingredients which you, the worker, need in order to calculate the block $S^{(i)}$. The other ingredients you need are the $\\mathbf{q}$ vectors which you, the worker, are holding. (Throughout the whole algorithm, each worker keeps hold of the same $\\mathbf{q}$'s.)\n",
        "\n",
        "By this process each worker gradually builds up their output vectors $\\mathbf{O}$.\n",
        "\n",
        "Let's look at a simplified version, where all the block sizes are 1. It should remind you of the simple ring reduce above, in many ways:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViXWU4-JsoIu",
        "outputId": "80065098-0e79-43f6-a01e-865dafa9d1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "naive: tensor([[-1.0470,  0.4340,  0.2762,  0.6966],\n",
            "        [-0.0038,  0.0370,  0.6868,  0.1482],\n",
            "        [-0.3025,  0.1263,  0.7148,  1.4139],\n",
            "        [-0.7548,  0.1541,  0.5451,  1.1918],\n",
            "        [-1.2017,  0.6329,  0.1033,  0.4751]])\n",
            "ring: tensor([[-1.0470,  0.4340,  0.2762,  0.6966],\n",
            "        [-0.0038,  0.0370,  0.6868,  0.1482],\n",
            "        [-0.3025,  0.1263,  0.7148,  1.4139],\n",
            "        [-0.7548,  0.1541,  0.5451,  1.1918],\n",
            "        [-1.2017,  0.6329,  0.1033,  0.4751]])\n",
            "incremental: tensor([[-1.0470,  0.4340,  0.2762,  0.6966],\n",
            "        [-0.0038,  0.0370,  0.6868,  0.1482],\n",
            "        [-0.3025,  0.1263,  0.7148,  1.4139],\n",
            "        [-0.7548,  0.1541,  0.5451,  1.1918],\n",
            "        [-1.2017,  0.6329,  0.1033,  0.4751]])\n",
            "delta: tensor([[False, False, False, False],\n",
            "        [False, False, False, False],\n",
            "        [False, False, False, False],\n",
            "        [False, False, False, False],\n",
            "        [False, False, False, False]])\n",
            "delta naive_attn vs naive_ring_attn: tensor(1.0058e-06)\n",
            "delta naive_attn vs naive_attn_incremental: tensor(8.9407e-07)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def update(m, l, O, s, v):\n",
        "      m_new = torch.max(m, s)\n",
        "      l_new = torch.exp(m - m_new) * l + torch.exp(s - m_new)\n",
        "      p = (1 / l_new) * torch.exp(s - m_new)\n",
        "      O_new = (l_new / l).pow(-1) * O * torch.exp(m - m_new) + p * v\n",
        "      return m_new, l_new, O_new\n",
        "\n",
        "def naive_attn(q, k, v, scale):\n",
        "    s = q @ k.mT * scale\n",
        "    a = torch.softmax(s, dim=-1)\n",
        "    return a @ v\n",
        "\n",
        "def naive_ring_attn(q, k, v, scale):\n",
        "    n = q.size(0)\n",
        "    O = torch.zeros_like(v)\n",
        "    m = torch.full(size = (n,), fill_value=torch.finfo(q.dtype).min)\n",
        "    l = torch.zeros_like(m)\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            #\n",
        "            # At time stage i, worker j:\n",
        "            #\n",
        "            # - Starts out holding k and v vectors both with indices\n",
        "            #   (j - 1) % N\n",
        "            #\n",
        "            # - Computes the dot product of its q (i.e. q_j) with the\n",
        "            #   k it's currently holding\n",
        "\n",
        "            k_index = (i + j - 1) % n\n",
        "            s = torch.dot(q[j], k[k_index]) * scale\n",
        "            m[j], l[j], O[j] = update(m[j], l[j], O[j], s, v[k_index])\n",
        "\n",
        "            # - If the time stage is not already (n - 1), we imagine\n",
        "            #   sending the k and v we're currently holding to our\n",
        "            #   neighbor in the ring, worker (j - 1) % n, and\n",
        "            #   receiving a new k and v from our other neighbor,\n",
        "            #   worker (j + 1) % n\n",
        "    return O\n",
        "\n",
        "def naive_attn_incremental(q, k, v, scale):\n",
        "    n = q.size(0)\n",
        "    O = torch.zeros_like(v)\n",
        "    for i in range(n):\n",
        "        m = torch.tensor(torch.finfo(q.dtype).min)\n",
        "        l = torch.tensor(0.0)\n",
        "        for j in range(n):\n",
        "            \n",
        "            s = torch.dot(q[i, : ], k[j, :]) * scale\n",
        "            m, l, O[i, : ] = update(m, l, O[i, :], s, v[j, : ])\n",
        "\n",
        "    return O\n",
        "\n",
        "\n",
        "n = 5\n",
        "d = 3\n",
        "d_v = 4\n",
        "\n",
        "k = torch.randn(n, d)\n",
        "q = torch.randn(n, d)\n",
        "v = torch.randn(n, d_v)\n",
        "scale = d**-0.5\n",
        "\n",
        "o1 = naive_attn(q.clone(), k.clone(), v.clone(), scale)\n",
        "o2 = naive_ring_attn(q.clone(), k.clone(), v.clone(), scale)\n",
        "o3 = naive_attn_incremental(q.clone(), k.clone(), v.clone(), scale)\n",
        "print(\"naive:\", o1)\n",
        "print(\"ring:\", o2)\n",
        "print(\"incremental:\", o3)\n",
        "print(\"delta:\", torch.abs(o1 - o2)>1e-6)\n",
        "\n",
        "print(\"delta naive_attn vs naive_ring_attn:\", torch.abs(o1 - o2).sum())\n",
        "print(\"delta naive_attn vs naive_attn_incremental:\", torch.abs(o1 - o3).sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTcMXYVwJLi"
      },
      "source": [
        "Both results `naive_ring_attn` and `naive_attn_inceremental` are close to out reference `naive_attn` result."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
